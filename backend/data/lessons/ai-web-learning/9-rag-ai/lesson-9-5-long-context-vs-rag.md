# Урок 9.5: Длинный контекст vs RAG

> **Модуль 9:** RAG + AI Агент  
> **Урок:** 9.5  
> **Длительность:** 90 минут  
> **Prerequisite:** Уроки 9.0-9.4

---

## 🎯 Цели урока

После этого урока ты сможешь:
- ✅ Понимать два подхода: RAG vs Длинный контекст
- ✅ Знать когда использовать каждый подход
- ✅ Сравнивать стоимость для твоего курса (350K токенов)
- ✅ Выбрать оптимальную модель (Gemini 2.5 Flash)
- ✅ Принять архитектурное решение для образовательной платформы

---

## 📖 Часть 1: Два подхода к AI агенту

### Главный вопрос урока

**"Как дать AI агенту доступ к моему курсу на 350,000 токенов?"**

### Два решения

**Подход 1: RAG (Retrieval-Augmented Generation)**
```
Студент задаёт вопрос
    ↓
Векторная БД ищет релевантные части курса
    ↓
Передаём AI только найденные части (~30K токенов)
    ↓
AI отвечает на основе найденного
```

**Подход 2: Длинный контекст (Long Context)**
```
Студент задаёт вопрос
    ↓
Передаём AI ВЕСЬ курс (350K токенов) сразу
    ↓
AI видит всё и отвечает
```

---

## 🏪 Аналогия: Библиотека vs Энциклопедия

### RAG = Библиотекарь с каталогом

```
┌─────────────────────────────────────┐
│         БИБЛИОТЕКА (RAG)            │
├─────────────────────────────────────┤
│                                     │
│  👨‍🏫 Библиотекарь (Vector Search)    │
│                                     │
│  Студент: "Что такое FastAPI?"      │
│         ↓                           │
│  Библиотекарь ищет в каталоге       │
│         ↓                           │
│  Достаёт 3 книги про FastAPI        │
│  (только релевантное)               │
│         ↓                           │
│  Студент читает найденное           │
│                                     │
│  📚 Все книги: 1000 штук            │
│  📖 Выдано: 3 книги                 │
│                                     │
└─────────────────────────────────────┘
```

**Плюсы:**
- ✅ Дёшево (передаём мало текста)
- ✅ Быстро (меньше обрабатывать)

**Минусы:**
- ❌ Может пропустить связи между темами
- ❌ Сложная реализация (векторная БД, embeddings)

---

### Длинный контекст = Энциклопедия в голове

```
┌─────────────────────────────────────┐
│    ДЛИННЫЙ КОНТЕКСТ (Long Context)  │
├─────────────────────────────────────┤
│                                     │
│  🧠 AI с энциклопедией в памяти     │
│                                     │
│  Студент: "Что такое FastAPI?"      │
│         ↓                           │
│  AI уже ЗНАЕТ весь курс наизусть    │
│  (все 1000 страниц в памяти)        │
│         ↓                           │
│  Находит ответ мгновенно            │
│  + видит связи с другими темами     │
│                                     │
│  📚 Все знания: 1000 страниц        │
│  🧠 В памяти: ВСЁ сразу             │
│                                     │
└─────────────────────────────────────┘
```

**Плюсы:**
- ✅ Видит ВСЕ связи между темами
- ✅ Простая реализация (один API вызов)
- ✅ Лучшее качество ответов

**Минусы:**
- ❌ Дороже (передаём много текста)
- ❌ Медленнее (больше обрабатывать)

---

## 💡 Контекст применения: Когда что использовать?

### Используй RAG когда:

**✅ Сценарии для RAG:**
- Огромный контент (\u003e5M токенов, 2500+ страниц)
- Контент постоянно обновляется (новости, документация)
- Бюджет критичен (нужна максимальная экономия)
- Простые вопросы (факты, определения)
- Контент плохо структурирован

**Пример:**
```
База знаний компании: 100,000 документов
→ RAG найдёт нужные 10 документов
→ Экономия: передаём 0.01% вместо 100%
```

---

### Используй Длинный контекст когда:

**✅ Сценарии для Long Context:**
- Курс/книга фиксированного размера (≤1M токенов)
- Контент стабильный (обновляется редко)
- Нужны связи между темами (обучение, исследования)
- Сложные вопросы требующие понимания всего контекста
- Простота реализации важна (MVP за выходные)

**Пример:**
```
Образовательный курс: 350K токенов (175,000 слов)
→ Весь курс помещается в 1M контекст
→ AI видит ВСЁ → лучшие ответы
```

---

## 🔄 Сравнение подходов для твоего курса

### Твоя ситуация: 350K токенов курса веб-разработки

| Критерий | RAG | Длинный контекст |
|----------|-----|------------------|
| **Размер контекста** | ~30K (релевантное) | 350K (всё) |
| **Качество ответов** | ⚠️ Хорошее | ✅ Отличное |
| **Видит связи тем** | ❌ Нет | ✅ Да |
| **Стоимость (100 студентов)** | ~$120/мес | ~$290-350/мес |
| **Сложность реализации** | 🔴 Высокая | 🟢 Низкая |
| **Время разработки** | 2-3 недели | 2-3 дня |
| **Maintenance** | Сложный (векторная БД) | Простой (один API) |

---

### Для твоего образовательного курса:

**🎯 РЕКОМЕНДАЦИЯ: Длинный контекст (Gemini 2.5 Flash)**

**Почему:**
1. ✅ **350K легко влезает** в 1M контекст модели
2. ✅ **Лучшее качество** - AI видит все связи между темами курса
3. ✅ **Быстрая разработка** - MVP за выходные вместо недель
4. ✅ **Приемлемая цена** - $2.90-3.50 за студента/месяц
5. ✅ **Prompt caching** - экономит 90% на повторных запросах

**Курс стабильный:**
- Уроки меняются редко
- Структурирован (модули 1-10)
- Студенты задают сложные вопросы про связи тем
- RAG может пропустить важные концепции из других модулей

---

## 🌐 Часть 2: Выбор модели для длинного контекста

### Топ-3 модели для твоего сценария

На основе исследования (октябрь 2025):

#### 1. Gemini 2.5 Flash ⭐ (РЕКОМЕНДУЮ)

**Характеристики:**
- Контекст: 1M токенов
- Цена входные: $0.30 за 1M токенов
- Цена выходные: $2.50 за 1M токенов
- Prompt caching: 90% скидка ($0.075 за 1M)

**Стоимость для 100 студентов:**
- Без кэша: $882/месяц
- С кэшем: **$290-350/месяц**
- **За студента: $2.90-3.50/месяц**

**Плюсы:**
- ✅ Самая дешёвая среди качественных
- ✅ 90% экономия на кэше (лучшая в отрасли)
- ✅ Бесплатный tier для тестирования
- ✅ Отличная для технического контента
- ✅ Поддержка русского языка

**Минусы:**
- ⚠️ Хранение кэша: $0.35/час ($252/мес при 24/7)

---

#### 2. GPT-4.1 Mini (альтернатива)

**Характеристики:**
- Контекст: 1M токенов
- Цена входные: $0.40 за 1M
- Цена выходные: $1.60 за 1M
- Prompt caching: 75% скидка ($0.10 за 1M)

**Стоимость для 100 студентов:**
- Без кэша: $1,168/месяц
- С кэшем: **$540-650/месяц**
- **За студента: $5.40-6.50/месяц**

**Плюсы:**
- ✅ Проверенная надёжность OpenAI
- ✅ Автоматическое кэширование
- ✅ Отличное следование инструкциям

**Минусы:**
- ❌ Дороже чем Gemini (в 2 раза)
- ⚠️ Кэш живёт 5-10 минут (короче)

---

#### 3. Claude Sonnet 4.5 (премиум)

**Характеристики:**
- Контекст: 1M токенов (бета)
- Цена входные: $6.00 за 1M (\u003e200K)
- Цена выходные: $22.50 за 1M
- Prompt caching: 90% скидка ($0.30 за 1M)

**Стоимость для 100 студентов:**
- Без кэша: $17,513/месяц 😱
- С кэшем: **$730-950/месяц**
- **За студента: $7.30-9.50/месяц**

**Плюсы:**
- ✅ Лучшее качество для кодинга
- ✅ Превосходное глубокое рассуждение

**Минусы:**
- ❌ Дорого (в 3 раза дороже Gemini)
- ⚠️ Требуется Tier 4+ ($400 в кредитах)

---

## 🎯 Финальная рекомендация: Gemini 2.5 Flash

### Почему именно Gemini 2.5 Flash?

```
┌────────────────────────────────────────────────┐
│         СРАВНЕНИЕ ПО КРИТЕРИЯМ                 │
├────────────────────────────────────────────────┤
│                                                │
│  💰 Стоимость за студента:                     │
│  Gemini 2.5 Flash:  $2.90-3.50 ✅ ЛУЧШЕ        │
│  GPT-4.1 Mini:      $5.40-6.50 ⚠️  ДОРОЖЕ      │
│  Claude Sonnet 4.5: $7.30-9.50 ❌ ДОРОГО       │
│                                                │
│  ⚡ Скорость кэширования:                      │
│  Gemini:  90% экономия ✅ ЛУЧШЕ                │
│  GPT:     75% экономия ⚠️  ХОРОШО              │
│  Claude:  90% экономия ✅ ЛУЧШЕ                │
│                                                │
│  🚀 Простота старта:                           │
│  Gemini:  Free tier, без карты ✅ ЛЕГКО        │
│  GPT:     Нужна карта сразу    ⚠️  СРЕДНЕ      │
│  Claude:  $400 мин. кредиты    ❌ СЛОЖНО       │
│                                                │
│  📚 Качество для техконтента:                  │
│  Gemini:  Отличное ✅                          │
│  GPT:     Отличное ✅                          │
│  Claude:  Превосходное ⭐                      │
│                                                │
└────────────────────────────────────────────────┘

ВЕРДИКТ: Gemini 2.5 Flash — лучшее соотношение
         цена/качество для образовательного курса
```

---

## 🛠️ Часть 3: Как работает Prompt Caching

### Что такое Prompt Caching?

**Простое определение:**
Prompt Caching = модель запоминает части промпта и не обрабатывает их заново при повторных запросах.

### Аналогия: Кэш как закладка в книге

**Без кэша:**
```
Студент 1 спрашивает: "Что такое FastAPI?"
→ AI читает ВСЮ книгу (350K) → находит ответ
→ Стоимость: $0.30 за 1M = $0.105

Студент 2 спрашивает: "Что такое React?"
→ AI СНОВА читает ВСЮ книгу (350K) → находит ответ
→ Стоимость: ещё $0.105

Студент 3...
→ И так далее ❌ ДОРОГО
```

**С кэшем:**
```
Студент 1 спрашивает: "Что такое FastAPI?"
→ AI читает книгу (350K) → ставит ЗАКЛАДКУ
→ Стоимость: $0.105

Студент 2 спрашивает: "Что такое React?"
→ AI открывает по ЗАКЛАДКЕ → уже знает контекст
→ Стоимость: $0.075 за 1M = $0.026 (в 4 раза дешевле!)

Студент 3...
→ Тоже по закладке ✅ ЭКОНОМИЯ 90%
```

---

### Как настроить Prompt Caching в Gemini

**Структура промпта:**

```
┌──────────────────────────────────────┐
│     КЭШИРУЕМАЯ ЧАСТЬ (статичная)     │
├──────────────────────────────────────┤
│                                      │
│  Системный промпт:                   │
│  "Ты - AI наставник курса..."        │
│                                      │
│  Весь курс веб-разработки:           │
│  Модуль 1: Основы...                 │
│  Модуль 2: Backend...                │
│  ...                                 │
│  (350,000 токенов)                   │
│                                      │
│  ← КЭШ АКТИВИРУЕТСЯ ЗДЕСЬ            │
│                                      │
└──────────────────────────────────────┘
         ↓
┌──────────────────────────────────────┐
│   НЕКЭШИРУЕМАЯ ЧАСТЬ (динамичная)    │
├──────────────────────────────────────┤
│                                      │
│  История диалога:                    │
│  Студент: "Привет!"                  │
│  AI: "Здравствуй!"                   │
│                                      │
│  Новый вопрос:                       │
│  Студент: "Что такое FastAPI?"       │
│                                      │
└──────────────────────────────────────┘
```

**Ключевые правила:**
1. Статичный контент (курс) → В НАЧАЛЕ промпта
2. Динамичный контент (вопросы) → В КОНЦЕ промпта
3. Минимум 1,024 токена для активации кэша
4. Любое изменение кэшируемой части → новый кэш (дорого!)

---

## 💰 Расчёт стоимости для твоего курса

### Сценарий: 100 студентов, смешанный паттерн

**Паттерн использования:**
- 70% коротких сессий (1-3 вопроса)
- 30% длинных сессий (20-30 вопросов)
- Среднее: 83 загрузки курса на студента/месяц

**Расчёт для Gemini 2.5 Flash:**

```
БЕЗ кэширования:
─────────────────
Входные: 2.9B токенов × $0.30/1M = $870
Выходные: 5M токенов × $2.50/1M = $12.50
ИТОГО: $882.50/месяц
На студента: $8.83/месяц

С prompt caching (90% экономия):
──────────────────────────────────
Первая загрузка курса (8,300 раз): 
  350K × 8,300 × $0.30/1M = $870

Кэшированные повторные запросы (~70%):
  2.0B × $0.075/1M = $150

Некэшированные вопросы (~30%):
  0.9B × $0.30/1M = $270

Выходные:
  5M × $2.50/1M = $12.50

ИТОГО: $290-350/месяц ✅
На студента: $2.90-3.50/месяц ✅
Экономия: 60-67% vs без кэша
```

---

## 🔍 Часть 4: RAG vs Long Context - финальное сравнение

### Что если использовать RAG?

**RAG подход для твоего курса:**
```
1. Векторная БД с курсом (OpenAI embeddings)
2. На вопрос → поиск топ-3 релевантных урока
3. Передача ~30K токенов вместо 350K
4. AI отвечает на основе найденного
```

**Стоимость RAG (Gemini 2.5 Flash):**
```
Входные: 35K × 10,000 запросов = 350M × $0.30/1M = $105
Embeddings: бесплатно (Gemini Embedding API)
Выходные: 5M × $2.50/1M = $12.50
ИТОГО: ~$120/месяц

Экономия vs Long Context: $230/месяц (66%)
```

**НО!** Есть скрытые затраты RAG:
- Векторная БД: ~$25-50/мес (pgvector хостинг)
- Разработка: 2-3 недели вместо 2-3 дней
- Maintenance: сложнее (индексация, мониторинг)

**Компромисс качества:**
```
Long Context видит:
✅ Модуль 1: Client-Server
✅ Модуль 2: FastAPI
✅ Модуль 8: Next.js
→ Может ответить: "FastAPI используй для Python, 
   Next.js для JavaScript, выбор зависит от задачи"

RAG видит (если вопрос про FastAPI):
✅ Модуль 2: FastAPI
❌ Модуль 8: Next.js (не найдёт, не релевантно)
→ Не сможет сравнить подходы
```

---

### Когда RAG лучше для образовательной платформы?

**Используй RAG если:**
- Курс \u003e 5M токенов (1,000+ уроков)
- Постоянно добавляются новые уроки
- Бюджет \u003c $100/месяц критичен
- Вопросы простые (факты, определения)

**Для твоего курса 350K токенов:**
→ **Long Context лучше** ✅

---

## 📝 Резюме урока

### Два подхода

```
RAG:
├─ Передаём только релевантное (30K)
├─ Дешевле (~$120/мес)
├─ Сложная реализация (2-3 недели)
└─ Может пропустить связи тем

Long Context:
├─ Передаём ВСЁ (350K)
├─ Приемлемая цена ($290-350/мес)
├─ Простая реализация (2-3 дня)
└─ Видит ВСЕ связи тем ✅
```

### Для твоего курса: Gemini 2.5 Flash

```
Модель: Gemini 2.5 Flash
Подход: Длинный контекст (1M)
Стоимость: $2.90-3.50 за студента/месяц
Качество: Отличное для техконтента
Простота: MVP за выходные
```

### Prompt Caching экономит 90%

```
Без кэша: $8.83/студент
С кэшем: $2.90-3.50/студент
Экономия: $5.93-6.00 (67%)
```

---

## ❓ Вопросы для самопроверки

1. **В чём разница между RAG и Long Context?**
   - Ответ: RAG передаёт только релевантное, Long Context - весь контент

2. **Когда использовать Long Context?**
   - Ответ: Когда контент ≤1M токенов, стабильный, нужны связи между темами

3. **Почему Gemini 2.5 Flash лучше для твоего курса?**
   - Ответ: Дешевле ($2.90/студент), 90% экономия на кэше, бесплатный tier

4. **Что такое Prompt Caching?**
   - Ответ: Модель запоминает статичные части промпта и не обрабатывает их заново

5. **Сколько стоит RAG vs Long Context для 100 студентов?**
   - Ответ: RAG ~$120/мес, Long Context ~$290-350/мес (но проще и качественнее)

---

## 🔗 Связь с другими уроками

**Основано на уроках:**
- Урок 9.0-9.4: Понимание RAG подхода
- Весь модуль 9: Контекст применения AI агентов

**Подготавливает к урокам:**
- Урок 9.6: Создание AI агента с Gemini 2.5 Flash
- Урок 9.7: Production оптимизация

**Связь с проектом:**
Этот урок помогает принять архитектурное решение для твоей образовательной платформы - использовать Long Context подход с Gemini 2.5 Flash для максимального качества при приемлемой цене.

---

## ✅ Критерии завершения урока

**Понимание:**
- [ ] Понимаю разницу между RAG и Long Context
- [ ] Знаю когда использовать каждый подход
- [ ] Понимаю как работает Prompt Caching
- [ ] Могу рассчитать стоимость для моего проекта

**Решение:**
- [ ] Выбрал подход для своей платформы (Long Context)
- [ ] Выбрал модель (Gemini 2.5 Flash)
- [ ] Понимаю структуру промпта с кэшированием

**Готовность:**
- [ ] Готов к Уроку 9.6 (реализация AI агента)
- [ ] Понимаю ожидаемую стоимость ($290-350/мес для 100 студентов)

---

**Статус урока:** ⏸️ Не начат  
**Следующий урок:** 9.6 - AI агент с Gemini 2.5 Flash

**Конец урока 9.5** ✅